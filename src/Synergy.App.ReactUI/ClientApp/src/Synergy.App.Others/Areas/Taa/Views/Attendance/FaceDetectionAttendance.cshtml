@using Synergy.App.Common;

@model IUserContext;

@{
    Layout = "~/Areas/Core/Views/Shared/_PopupLayout.cshtml";
}

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/css/materialize.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/js/materialize.min.js"></script>
<link href="~/js/FaceAPI/styles.css" rel="stylesheet" />

<script>

    var faceDetectionOptions = new faceapi.TinyFaceDetectorOptions({ inputSize, scoreThreshold });
    var faceDetectionNet = faceapi.nets.tinyFaceDetector;
    var faceMatcher;
    var withBoxes = true;
    var caputreImageIndex = 1;
    var descriptors = [];
    var capturedCanvas = [];
    var labeldescriptors = [];

    async function onPlay() {
        if (isOn == true && isDetection == true) {
            const videoEl = $('#inputVideo').get(0)

            if (videoEl.paused || videoEl.ended || !isFaceDetectionModelLoaded())
                return setTimeout(() => onPlay())

            if (caputreImageIndex < 6) {
                const options = getFaceDetectorOptions()

                const useTinyModel = true
                //const result = await faceapi.detectSingleFace(videoEl, options).withFaceLandmarks(useTinyModel)
                const result = await faceapi.detectSingleFace(videoEl, options).withFaceLandmarks(useTinyModel)
                    .withFaceDescriptor()


                const canvas = $('#overlay').get(0)


                //var res = await faceapi.detectAllFaces(videoEl, options).withFaceLandmarks(useTinyModel)
                //const faceImages = await faceapi.extractFaces(videoEl, res)
                //    displayExtractedFaces(faceImages);
                //
                //updateTimeStats(Date.now() - ts)

                if (result) {
                    descriptors.push(result.descriptor);
                    const canvas = $('#overlay').get(0)
                    const dims = faceapi.matchDimensions(canvas, videoEl, true)
                    const resizedResult = faceapi.resizeResults(result, dims)

                    if (withBoxes) {
                        //faceapi.draw.drawDetections(canvas, resizedResult)
                    }
                    //faceapi.draw.drawFaceLandmarks(canvas, resizedResult)
                    drawFaceRecognitionResults(result);

                }

                setTimeout(async () => { await onPlay() })
            } else {
                HideLoader($('#loaderca'));
            }
        }
    }

    function drawFaceRecognitionResults(results) {
        const canvas = $('#overlay').get(0)
        if (results) {

            const bestMatch = faceMatcher.findBestMatch(results.descriptor);
            var label = bestMatch.label;
            if (bestMatch.distance <= scoreThreshold) {
                const options = { label }
                const drawBox = new faceapi.draw.DrawBox(results.alignedRect.box, options)
                drawBox.draw(canvas)
                isMatch = true;
                label = "face matched with score (" + bestMatch.distance + ") -" + bestMatch.label;
                //ShowNotification(label, "success");
            } else {
                ShowNotification("face not matched", "error");
            }
        }
        HideLoader($('#compute'));
    }


    $(document).ready(function () {
        loadData()
    })

    async function run() {
        await faceapi.loadTinyFaceDetectorModel('/')
        await faceapi.loadFaceLandmarkTinyModel('/')
        await faceapi.loadFaceRecognitionModel('/')
        const stream = await navigator.mediaDevices.getUserMedia({ video: {} })
        const videoEl = $('#inputVideo').get(0)
        videoEl.srcObject = stream
    }

    var userDescriptors = []
    async function loadData() {
        ShowLoader($('#loaderca'));
        await faceapi.loadTinyFaceDetectorModel('/')
        await faceapi.loadFaceLandmarkTinyModel('/')
        await faceapi.loadFaceRecognitionModel('/')

        await $.ajax({
            url: '/cms/user/GetFaceDataModel',
            dataType: "json",
            success: async function (result) {

                if (result != "" && result != null) {

                    for (var i = 0; i <= result.length - 1; i++) {
                        var res = decodeDescriptors(result[i].Code);
                        if (res != null) {
                            const filtered = res.reduce((a, o) => (a.push(o.data), a), []);
                            for (var x = 0; x < filtered.length; x++) {
                                var results = filtered[x];
                                filtered[x] = new Float32Array(results);
                            }

                            var labeledFaceDescriptorobj = new faceapi.LabeledFaceDescriptors(
                                result[i].Name,
                                filtered
                            )
                            userDescriptors.push(labeledFaceDescriptorobj)
                        }
                    }
                    document.getElementById("load").style.display = "none";
                    ShowNotification("Model loaded Successfully", "success");
                    HideLoader($('#loaderca'));
                    faceMatcher = new faceapi.FaceMatcher(userDescriptors, scoreThreshold);
                    document.getElementById("waterMark").style.display = "";

                    if (isOn == true) {
                        //await run()
                    }
                } else {
                    alert("No models found to match the face");
                }

            }
        });
    }


    function decodeDescriptors(jsonStr) {
        var decodedJson = JSON.parse(jsonStr, function (key, value) {
            try {
                return new context[value.constructor](value.data);
            } catch (e) { }
            return value;
        });
        return decodedJson;
    }

    var vid = document.getElementById("inputVideo");
    async function onCamera() {
        document.getElementById("waterMark").style.display = "none";
        document.getElementById("inputVideo").style.display = "";

        //await run();
        navigator.mediaDevices.getUserMedia({
            audio: true,
            video: true
        })
            .then(stream => {
                var vid = document.getElementById("inputVideo");

                window.localStream = stream;
                vid.srcObject = stream;
                isOn = true;
                //audio.srcObject = stream;
            })
            .catch((err) => {
                console.log(err);
            });
    }
    var isOn = false;

    async function offCamera() {
        var vid = document.getElementById("inputVideo");

        document.getElementById("waterMark").style.display = "";

        localStream.getVideoTracks()[0].stop();
        vid.src = '';
        isOn = false;
    }
    var isDetection = false;
    async function DetectFace() {
        isDetection = true;
        if (isOn == true) {
            await run();
            //  await onPlay();
        } else {
            ShowNotification("Camera is Off, Please switch on your camera", "error")

        }
    }

    function stopdetectingFace() {
        isDetection = false;
        isOn = false;
        const video = document.querySelector('video');

        // A video's MediaStream object is available through its srcObject attribute
        const mediaStream = video.srcObject;

        // Through the MediaStream, you can get the MediaStreamTracks with getTracks():
        const tracks = mediaStream.getTracks();

        // Tracks are returned as an array, so if you know you only have one, you can stop it with:
        tracks[0].stop();

        // Or stop all like so:
        tracks.forEach(track => track.stop())
    }


</script>

<div class="row" id="loaderca" style="margin-top:2%">

    <div class="col">
        <div id="waterMark" style="display: none; font-size:25px">  <span class="fas fa-camera" style="margin-right:1%"> </span>Switch On Camera</div>
        <span id="load">Loading Models</span>
        <video onloadedmetadata="onPlay(this)" id="inputVideo" autoplay muted playsinline></video>
        <canvas class="transparent" id="overlay"></canvas>
        <div id="facesContainer"></div>
    </div>


    <div class="col">
        <button id="start" class="btn btn-primary" onclick="onCamera()"> On </button>
        <button id="off" class="btn btn-secondary" onclick="offCamera()"> Off </button>
        <button id="uploadCapturedImages" class="btn btn-primary" onclick="DetectFace()">Detect Face</button>
        <button id="stop" class="btn btn-primary" onclick="stopdetectingFace()">Stop Detect Face</button>
    </div>

</div>

<style>
    .transparent {
        background: rgba(255,255,255,0.5);
    }

    .capture-img {
        width: 100px;
        height: 100px;
    }

    canvas {
        height: 100px;
        width: 100px;
    }
</style>